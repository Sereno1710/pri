{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset = ir_datasets.load(\"cord19\")\n",
    "\n",
    "data = []\n",
    "\n",
    "for doc in dataset.docs_iter()[:20000]:  \n",
    " \n",
    "    data.append({\n",
    "        \"doc_id\": doc.doc_id,\n",
    "        \"title\": doc.title,\n",
    "        \"doi\": doc.doi,\n",
    "        \"date\": doc.date,\n",
    "        \"abstract\": doc.abstract\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"covid_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Dataset saved to covid_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [COVID-19 Open Research Dataset (CORD-19)](https://github.com/allenai/cord19) is a freely available resource of over 1,000,000 scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses. The dataset was developed by the Allen Institute for AI in collaboration with several organizations, including the White House, NIH, and leading research groups. Its goal is to facilitate the development of new tools and technologies to help researchers find relevant information about the virus and its spread, and to support the global research community in the fight against the pandemic. It was first published in March 2020 and has been updated weekly since then until May 2nd 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be obtained in various formats, including JSON, CSV. It contains metadata for each article, such as the id of the document, title, abstract, publication date. It has about 193,000 articles with a total size of 3.7GB since it uses the version of 16-07-2020. Some of these articles have missing abstracts or share the same title. This can happen because the dataset is a collection of articles from different sources, and some of them may have been published in more than one place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df = df[df[\"abstract\"] != \"\"]\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "np = [\n",
    "    ''.join([char for char in abs if char not in punctuation])\n",
    "    for abs in df[\"abstract\"]\n",
    "]\n",
    "print(np[:5])\n",
    "\n",
    "token_abs = [word_tokenize(abs) for abs in np]\n",
    "print(token_abs[:5])\n",
    "\n",
    "abs_nstp = [\n",
    "    ' '.join([word for word in abs if word.lower() not in stop_words])\n",
    "    for abs in token_abs\n",
    "]\n",
    "print(abs_nstp[:5])\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "abs_lem = [\n",
    "    ' '.join([lemmatizer.lemmatize(word) for word in abs.split()])\n",
    "    for abs in abs_nstp\n",
    "]\n",
    "print(abs_lem[:5])\n",
    "\n",
    "df['lem_abstract'] = abs_lem\n",
    "\n",
    "df.to_csv(\"covid_dataset_preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=100,\n",
    "    random_state=44,\n",
    "    max_font_size=110\n",
    ")\n",
    "wc.generate(' '.join(df['lem_abstract']))\n",
    "plt.figure(figsize=(50, 7))\n",
    "plt.imshow(wc)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "df['entities'] = df['lem_abstract'].apply(lambda x: extract_entities(str(x)))\n",
    "print(df[['entities']].head())\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(nlp(df['lem_abstract'].iloc[0]), style='ent', jupyter=True)\n",
    "## verificar sci-spacy para melhorar a extração de entidades nomeadas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
